{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34412004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1621292081500_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-96.eu-west-2.compute.internal:20888/proxy/application_1621292081500_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-40-79.eu-west-2.compute.internal:8042/node/containerlogs/container_1621292081500_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = 2.4.7-amzn-1\n"
     ]
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41d1fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
      "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
      "import org.apache.spark.ml.classification.RandomForestClassifier\n",
      "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
      "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}\n",
      "import org.apache.spark.ml.linalg.Vectors\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.log4j._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics  \n",
    "import org.apache.spark.ml.classification.RandomForestClassifier  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}  \n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoderEstimator}  \n",
    "import org.apache.spark.ml.linalg.Vectors  \n",
    "import org.apache.spark.ml.Pipeline  \n",
    "import org.apache.log4j._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f374d28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Logger.getLogger(\"org\").setLevel(Level.ERROR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c58739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.SparkSession\n",
      "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2e1e4f16\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ed2db",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Put the data into an S3 bucket and load it from there into a Spark DataFrame.\n",
    "\n",
    "Grab the dataset from the Harvard Dataverse: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26147\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26722cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: String = HXPC13_DI_v3_11-13-2019.csv\n",
      "bucket: String = s3://sagemaker-london/\n"
     ]
    }
   ],
   "source": [
    "var dataset=\"HXPC13_DI_v3_11-13-2019.csv\"\n",
    "var bucket=\"s3://sagemaker-london/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f91522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: org.apache.spark.sql.DataFrame = [course_id: string, userid_DI: string ... 18 more fields]\n"
     ]
    }
   ],
   "source": [
    "val data = spark.read.option(\"header\",\"true\").\n",
    "    option(\"inferSchema\",\"true\").\n",
    "    format(\"csv\").\n",
    "    load(bucket + dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96d96b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df = (data.select(data(\"certified\").as(\"label\"), \n",
    "          $\"registered\", $\"viewed\", $\"explored\", \n",
    "          $\"final_cc_cname_DI\", $\"gender\", $\"nevents\", \n",
    "          $\"ndays_act\", $\"nplay_video\", $\"nchapters\", $\"nforum_posts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8395ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexer1: org.apache.spark.ml.feature.StringIndexer = strIdx_78d7a9cadabf\n",
      "indexed1: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 10 more fields]\n",
      "indexer2: org.apache.spark.ml.feature.StringIndexer = strIdx_982088909bd7\n",
      "indexed2: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 11 more fields]\n",
      "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_7ed5d0900abb\n",
      "encoded: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "val indexer1 = new StringIndexer().\n",
    "    setInputCol(\"final_cc_cname_DI\").\n",
    "    setOutputCol(\"countryIndex\").\n",
    "    setHandleInvalid(\"keep\") \n",
    "val indexed1 = indexer1.fit(df).transform(df)\n",
    "\n",
    "val indexer2 = new StringIndexer().\n",
    "    setInputCol(\"gender\").\n",
    "    setOutputCol(\"genderIndex\").\n",
    "    setHandleInvalid(\"keep\")\n",
    "val indexed2 = indexer2.fit(indexed1).transform(indexed1)\n",
    "\n",
    "// one hot encoding\n",
    "val encoder = new OneHotEncoderEstimator().\n",
    "  setInputCols(Array(\"countryIndex\", \"genderIndex\")).\n",
    "  setOutputCols(Array(\"countryVec\", \"genderVec\"))\n",
    "val encoded = encoder.fit(indexed2).transform(indexed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334ebb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nanEvents: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [nevents: int, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "val nanEvents = encoded.groupBy(\"nevents\").count().orderBy($\"count\".desc)\n",
    "for (line <- nanEvents){\n",
    "    println(line)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0036c634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neventsMedianArray: Array[Double] = Array(17.0)\n",
      "neventsMedian: Double = 17.0\n",
      "ndays_actMedianArray: Array[Double] = Array(2.0)\n",
      "ndays_actMedian: Double = 2.0\n",
      "nplay_videoMedianArray: Array[Double] = Array(17.0)\n",
      "nplay_videoMedian: Double = 17.0\n",
      "nchaptersMedianArray: Array[Double] = Array(2.0)\n",
      "nchaptersMedian: Double = 2.0\n",
      "filled: org.apache.spark.sql.DataFrame = [label: int, registered: int ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "val neventsMedianArray = encoded.stat.approxQuantile(\"nevents\", Array(0.5), 0)\n",
    "val neventsMedian = neventsMedianArray(0)\n",
    "\n",
    "val ndays_actMedianArray = encoded.stat.approxQuantile(\"ndays_act\", Array(0.5), 0)\n",
    "val ndays_actMedian = ndays_actMedianArray(0)\n",
    "\n",
    "val nplay_videoMedianArray = encoded.stat.approxQuantile(\"nplay_video\", Array(0.5), 0)\n",
    "val nplay_videoMedian = nplay_videoMedianArray(0)\n",
    "\n",
    "val nchaptersMedianArray = encoded.stat.approxQuantile(\"nchapters\", Array(0.5), 0)\n",
    "val nchaptersMedian = nchaptersMedianArray(0)\n",
    "\n",
    "// replace \n",
    "val filled = encoded.na.fill(Map(\n",
    "  \"nevents\" -> neventsMedian, \n",
    "  \"ndays_act\" -> ndays_actMedian, \n",
    "  \"nplay_video\" -> nplay_videoMedian, \n",
    "  \"nchapters\" -> nchaptersMedian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "358c688d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_9a3c7dbdc7db\n",
      "output: org.apache.spark.sql.DataFrame = [label: int, features: vector]\n"
     ]
    }
   ],
   "source": [
    "// Set the input columns as the features we want to use\n",
    "val assembler = (new VectorAssembler().setInputCols(Array(\n",
    "  \"viewed\", \"explored\", \"nevents\", \"ndays_act\", \"nplay_video\", \n",
    "  \"nchapters\", \"nforum_posts\", \"countryVec\", \"genderVec\")).\n",
    "   setOutputCol(\"features\"))\n",
    "\n",
    "// Transform the DataFrame\n",
    "val output = assembler.transform(filled).select($\"label\",$\"features\")\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d93073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, features: vector]\n",
      "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: int, features: vector]\n"
     ]
    }
   ],
   "source": [
    "// Splitting the data by create an array of the training and test data\n",
    "val Array(training, test) = output.select(\"label\",\"features\").\n",
    "                            randomSplit(Array(0.7, 0.3), seed = 12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf8c0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_90b9f8aa8812\n",
      "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\trfc_90b9f8aa8812-numTrees: 20\n",
      "}, {\n",
      "\trfc_90b9f8aa8812-numTrees: 50\n",
      "}, {\n",
      "\trfc_90b9f8aa8812-numTrees: 100\n",
      "})\n",
      "cv: org.apache.spark.ml.tuning.CrossValidator = cv_fd6bca14c254\n",
      "model: org.apache.spark.ml.tuning.CrossValidatorModel = cv_fd6bca14c254\n"
     ]
    }
   ],
   "source": [
    "// create the model\n",
    "val rf = new RandomForestClassifier()\n",
    "\n",
    "// create the param grid\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "  addGrid(rf.numTrees,Array(20,50,100)).\n",
    "  build()\n",
    "\n",
    "// create cross val object, define scoring metric\n",
    "val cv = new CrossValidator().\n",
    "  setEstimator(rf).\n",
    "  setEvaluator(new MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\")).\n",
    "  setEstimatorParamMaps(paramGrid).\n",
    "  setNumFolds(3).\n",
    "  setParallelism(2)\n",
    "\n",
    "// You can then treat this object as the model and use fit on it.\n",
    "val model = cv.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735de80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: org.apache.spark.sql.DataFrame = [features: vector, label: int ... 1 more field]\n"
     ]
    }
   ],
   "source": [
    "val results = model.transform(test).select(\"features\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fadfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[694] at rdd at <console>:40\n"
     ]
    }
   ],
   "source": [
    "val predictionAndLabels = results.\n",
    "    select($\"prediction\",$\"label\").\n",
    "    as[(Double, Double)].\n",
    "    rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ff84bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bMetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@108a3e94\n",
      "mMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6922ea64\n",
      "labels: Array[Double] = Array(0.0, 1.0)\n",
      "Confusion matrix:\n",
      "99695.0  231.0  \n",
      "1215.0   709.0  \n"
     ]
    }
   ],
   "source": [
    "// Instantiate a new metrics objects\n",
    "val bMetrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "val mMetrics = new MulticlassMetrics(predictionAndLabels)\n",
    "val labels = mMetrics.labels\n",
    "\n",
    "// Print out the Confusion matrix\n",
    "println(\"Confusion matrix:\")\n",
    "println(mMetrics.confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1a7ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(0.0) = 0.9879595679318204\n",
      "Precision(1.0) = 0.7542553191489362\n",
      "Recall(0.0) = 0.9976882893341072\n",
      "Recall(1.0) = 0.3685031185031185\n",
      "FPR(0.0) = 0.6314968814968815\n",
      "FPR(1.0) = 0.0023117106658927604\n",
      "F1-Score(0.0) = 0.9928000956003903\n",
      "F1-Score(1.0) = 0.4951117318435754\n"
     ]
    }
   ],
   "source": [
    "// Precision by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Precision($l) = \" + mMetrics.precision(l))\n",
    "}\n",
    "\n",
    "// Recall by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Recall($l) = \" + mMetrics.recall(l))\n",
    "}\n",
    "\n",
    "// False positive rate by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"FPR($l) = \" + mMetrics.falsePositiveRate(l))\n",
    "}\n",
    "\n",
    "// F-measure by label\n",
    "labels.foreach { l =>\n",
    "  println(s\"F1-Score($l) = \" + mMetrics.fMeasure(l))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0cae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[713] at map at BinaryClassificationMetrics.scala:217\n",
      "recall: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[714] at map at BinaryClassificationMetrics.scala:217\n",
      "PRC: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[717] at union at BinaryClassificationMetrics.scala:110\n",
      "f1Score: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[718] at map at BinaryClassificationMetrics.scala:217\n",
      "beta: Double = 0.5\n",
      "fScore: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[719] at map at BinaryClassificationMetrics.scala:217\n",
      "auPRC: Double = 0.5220650321042403\n",
      "Area under precision-recall curve = 0.5220650321042403\n",
      "thresholds: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[725] at map at <console>:39\n",
      "roc: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[729] at UnionRDD at BinaryClassificationMetrics.scala:90\n",
      "auROC: Double = 0.6830957039186129\n",
      "Area under ROC = 0.6830957039186129\n"
     ]
    }
   ],
   "source": [
    "// Precision by threshold\n",
    "val precision = bMetrics.precisionByThreshold\n",
    "precision.foreach { case (t, p) =>\n",
    "  println(s\"Threshold: $t, Precision: $p\")\n",
    "}\n",
    "\n",
    "// Recall by threshold\n",
    "val recall = bMetrics.recallByThreshold\n",
    "recall.foreach { case (t, r) =>\n",
    "  println(s\"Threshold: $t, Recall: $r\")\n",
    "}\n",
    "\n",
    "// Precision-Recall Curve\n",
    "val PRC = bMetrics.pr\n",
    "\n",
    "// F-measure\n",
    "val f1Score = bMetrics.fMeasureByThreshold\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "}\n",
    "\n",
    "val beta = 0.5\n",
    "val fScore = bMetrics.fMeasureByThreshold(beta)\n",
    "f1Score.foreach { case (t, f) =>\n",
    "  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n",
    "}\n",
    "\n",
    "// AUPRC\n",
    "val auPRC = bMetrics.areaUnderPR\n",
    "println(\"Area under precision-recall curve = \" + auPRC)\n",
    "\n",
    "// Compute thresholds used in ROC and PR curves\n",
    "val thresholds = precision.map(_._1)\n",
    "\n",
    "// ROC Curve\n",
    "val roc = bMetrics.roc\n",
    "\n",
    "// AUROC\n",
    "val auROC = bMetrics.areaUnderROC\n",
    "println(\"Area under ROC = \" + auROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6c5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (Spark)",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
